{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URHO-Uji2koG"
      },
      "source": [
        "# M1 Assignment: Text Processing and Edit Distance\n",
        "\n",
        "In this section we will be exploring how to preprocess tweets . We will provide a function for preprocessing tweets during this week's assignment, but it is still good to know what is going on under the hood. By the end of this assignment, you will see how to use the [NLTK](http://www.nltk.org) package to perform a preprocessing pipeline for Twitter datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVg-26AV2koP"
      },
      "source": [
        "## Setup\n",
        "Eventually, you will conduct a sentiment analysis on Tweets. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. In this library you will use the NLTK to assist with processing the tweet to clean it for interpretation.\n",
        "\n",
        "In Part 1.1, you will extract a set of Elon Musk tweets. Next in Part 1.2, you will process the tweets using the various processing tasks outlined in the section and Chapter 2 of Jurafsky and Martin. Finally, in Part 1.3, you will create a simple version of a Levensthein distance formula to run the edit distance between two matrices.\n",
        "\n",
        "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n",
        "\n",
        "It will be look like following:\n",
        "```\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "'Some coding activity for you to complete'\n",
        "### END CODE HERE ###\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you will be using custom media and libraries created and stored for your in your Colab environment.\n"
      ],
      "metadata": {
        "id": "MYLzCpw62fYi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td35EC362koR"
      },
      "source": [
        "## Part 1.1: Using tweetsts\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzNDoS1P2koY"
      },
      "source": [
        "### Extracting tweets\n",
        "In this next section, we're going to import Elon Musk's tweets and prepare them to preprocess. Use the following code to connect to the M1 Assignment Repo and download it to your Google Colab Account\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install this if you do not have this already installed\n",
        "!pip install gitpython"
      ],
      "metadata": {
        "id": "cZPP74Tswh9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674f5dc0-b455-474c-bba6-27e97311909b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import git\n",
        "import sys\n",
        "\n",
        "\n",
        "# Clone the GitHub repository\n",
        "repo_url = 'https://github.com/Natural-Language-Processing-YU/Module-1-Assignment.git'\n",
        "repo_dir = '/content/m1_repo2'  # Specify the directory to clone the repository\n",
        "git.Repo.clone_from(repo_url, repo_dir)\n",
        "# Add the cloned repository directory to the import path\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WNrUiAYxglN",
        "outputId": "bbb29aa6-0a8e-496f-fbb6-cc6e52f88a43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<git.repo.base.Repo '/content/m1_repo2/.git'>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the pandas library to create a dataframe with the csv file full owf twees."
      ],
      "metadata": {
        "id": "Md_LSWJUud-U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W308paKm2kob",
        "outputId": "91d214b2-ad1b-4bd5-d483-7df247000cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#extract the tweets\n",
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('/content/m1_repo2/data/elonmusk_tweets.csv') #import file\n",
        "print(tweets.dtypes) #print data types\n",
        "print(tweets.text) #show tweets from file\n",
        "df = pd.DataFrame(tweets)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id             int64\n",
            "created_at    object\n",
            "text          object\n",
            "dtype: object\n",
            "0       b'And so the robots spared humanity ... https:...\n",
            "1       b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...\n",
            "2           b'@waltmossberg @mims @defcon_5 Et tu, Walt?'\n",
            "3                     b'Stormy weather in Shortville ...'\n",
            "4       b\"@DaveLeeBBC @verge Coal is dying due to nat ...\n",
            "                              ...                        \n",
            "2814                 b'That was a total non sequitur btw'\n",
            "2815    b'Great Voltaire quote, arguably better than T...\n",
            "2816    b'I made the volume on the Model S http://t.co...\n",
            "2817    b\"Went to Iceland on Sat to ride bumper cars o...\n",
            "2818    b'Please ignore prior tweets, as that was some...\n",
            "Name: text, Length: 2819, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2kk1YB-2koa"
      },
      "source": [
        "## Part 1.2: Preprocessing the text from tweets\n",
        "Text processing is one of the critical steps in an NLP project and in data scenience and analytics. It includes cleaning and formatting the data before feeding an algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
        "\n",
        "1. Tokenizing the string\n",
        "2. Lowercasing\n",
        "3. Removing stop words and punctuation\n",
        "4. Stemming\n",
        "\n",
        "We will take this approach with a selected tweet that we returned from above see how this is transformed by each preprocessing step.\n",
        "\n",
        "Let's take one of the tweets and apply preprocessing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at one tweet from our dataframe:"
      ],
      "metadata": {
        "id": "fry6Eqa9ryXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[92m' + df['text'][2816])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl2W0slVr2RS",
        "outputId": "292c9058-be16-445c-fa74-17cb97b9fc45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mb'I made the volume on the Model S http://t.co/wMCnT53M go to 11.  Now I just need to work in a miniature Stonehenge...'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ii7mmr2kod"
      },
      "source": [
        "### Remove hyperlinks, hashtags, and beginngin of strings\n",
        "Using this tweet, let's clean it up to remove unncessary information. First, we will use regex to remove hyperlinks. You will create the regex substring to remove hyperlinks, hashtabs and the start of a tweet line.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQljVSFM2kod",
        "outputId": "e0c71349-802a-4def-eef7-79a017563fb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "#create regex for hyperlinks\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Read in the dataset containing tweets\n",
        "tweets = pd.read_csv('/content/m1_repo2/data/elonmusk_tweets.csv')\n",
        "\n",
        "# Regular expression to eliminate URLs from the tweets\n",
        "regex_remove_hyperlinks = r'http\\S+'\n",
        "tweets['text'][2816] = re.sub(regex_remove_hyperlinks, '', tweets['text'][2816])\n",
        "\n",
        "# Regular expression for stripping the hash symbol from hashtags\n",
        "regex_hash = r'#'\n",
        "tweets['text'][2816] = re.sub(regex_hash, '', tweets['text'][2816])\n",
        "\n",
        "# Regular expression to delete byte string markers at the beginning of tweets\n",
        "regex_string_beginning = r\"^b['\\\"]\"\n",
        "tweets['text'][2816] = re.sub(regex_string_beginning, '', tweets['text'][2816])\n",
        "\n",
        "# Output the altered tweet using ANSI green color for emphasis\n",
        "print('\\033[92m' + tweets['text'][2816])\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mI made the volume on the Model S  go to 11.  Now I just need to work in a miniature Stonehenge...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e3ad3a3535cc>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  tweets['text'][2816] = re.sub(regex_remove_hyperlinks, '', tweets['text'][2816])\n",
            "<ipython-input-8-e3ad3a3535cc>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  tweets['text'][2816] = re.sub(regex_hash, '', tweets['text'][2816])\n",
            "<ipython-input-8-e3ad3a3535cc>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  tweets['text'][2816] = re.sub(regex_string_beginning, '', tweets['text'][2816])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using NLTK Libraries\n",
        "\n",
        "Now let's use the NLTK libraries to remove stopwords, tokenize, and stem the words.\n",
        "\n",
        "The Porter stemming algorithm, also known as the Porter stemmer, is a widely used algorithm for stemming words in natural language processing (NLP). It is named after its creator, Martin Porter. The goal of stemming is to reduce words to their base or root form, which helps to normalize variations of a word and reduce the vocabulary size."
      ],
      "metadata": {
        "id": "8X5-erZasQzI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZoSOGaW2koc"
      },
      "source": [
        "import nltk                                # Python library for NLP\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgaZcBmu2koc",
        "outputId": "ddf182d0-e300-4066-f5ee-bf510d1f33cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download the stopwords from NLTK\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBLZmjp2koe"
      },
      "source": [
        "### Tokenize the string\n",
        "\n",
        "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The [tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual) module from NLTK allows us to do these easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD2ub7Dl2koe",
        "outputId": "837b2bbd-2709-4e1c-fbff-0f9af8b583aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "tokenized_tweet = tokenizer.tokenize(str(df['text'][2816]))\n",
        "\n",
        "\n",
        "print(tokenized_tweet)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"b'i\", 'made', 'the', 'volume', 'on', 'the', 'model', 's', 'http://t.co/wMCnT53M', 'go', 'to', '11', '.', 'now', 'i', 'just', 'need', 'to', 'work', 'in', 'a', 'miniature', 'stonehenge', '...', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8jxMlK2kof"
      },
      "source": [
        "### Remove stop words and punctuations\n",
        "\n",
        "The next step is to remove stop words and miscelleneous punctuations. Stop words are words that do not have semantic meaning to the tweet. There is a library of stopwords built into NLTK. The list provided by NLTK when you run the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMOqHRp2kof",
        "outputId": "d4a35d00-8539-4f96-815a-cc7f75a202df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23rvPhYz2kof"
      },
      "source": [
        "We can see that the stop words list above contains some words that could be important in some contexts.\n",
        "These could be words like _i, not, between, because, won, against_. In some cases, you may want to update this dictionary of stop words to suit your needs.\n",
        "\n",
        "Certain groupings like ':)' and '...'  should be retained when dealing with tweets because they are used to express emotions, but in some cases they should be removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jnynu5K2kog",
        "outputId": "e19af1df-2c19-47fa-b98b-acb732279f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2=df\n",
        "tweets_clean = []\n",
        "\n",
        "for word in tokenized_tweet: # Go through every word in your tokens list\n",
        "    if (word not in stopwords_english and  # remove stopwords\n",
        "        word not in string.punctuation):  # remove punctuation\n",
        "        tweets_clean.append(word)\n",
        "\n",
        "print('removed stop words and punctuation:')\n",
        "print(tweets_clean)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed stop words and punctuation:\n",
            "[\"b'i\", 'made', 'volume', 'model', 'http://t.co/wMCnT53M', 'go', '11', 'need', 'work', 'miniature', 'stonehenge', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use porter stemmer to stem words.\n",
        "\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)"
      ],
      "metadata": {
        "id": "Wse80ZKUrASY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "for word in tokenized_tweet:\n",
        "    stem_word = stemmer.stem(word)\n",
        "    tweets_clean.append(stem_word)\n",
        "print(tweets_clean)"
      ],
      "metadata": {
        "id": "Nwq8pacZrLXg",
        "outputId": "53117727-4011-4aba-ae5b-f8936b08b92a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"b'i\", 'made', 'volume', 'model', 'http://t.co/wMCnT53M', 'go', '11', 'need', 'work', 'miniature', 'stonehenge', '...', \"b'i\", 'made', 'the', 'volum', 'on', 'the', 'model', 's', 'http://t.co/wmcnt53m', 'go', 'to', '11', '.', 'now', 'i', 'just', 'need', 'to', 'work', 'in', 'a', 'miniatur', 'stoneheng', '...', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9quRejfi2kog"
      },
      "source": [
        "## preprocess_tweet()\n",
        "\n",
        "As shown above, preprocessing consists of multiple steps before you arrive at the final list of words.  In the week's assignment, write a function called `preprocess_tweet()`.\n",
        "\n",
        "Then, use this to iterate through the dataframe and preprocess each tweet into a new column.\n",
        "\n",
        "Print your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frUPXf542koh",
        "outputId": "e8cb03c5-ad28-4911-bf68-de631e4337bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "\n",
        "# Make sure the necessary NLTK resources are available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Setting up NLTK tools for text processing\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "stopwords_english = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"\n",
        "    Enhances the quality of tweet text by removing noise and reducing words to their base forms.\n",
        "    Input:\n",
        "        tweet: String, original tweet text.\n",
        "    Output:\n",
        "        tweets_clean: List, processed words from the original tweet.\n",
        "    \"\"\"\n",
        "    # Begin with tokenization to break down the tweet into manageable parts\n",
        "    tokenized_tweet = tokenizer.tokenize(tweet)\n",
        "\n",
        "    # List to hold the cleaned words\n",
        "    tweets_clean = []\n",
        "\n",
        "    # Filter out stopwords and punctuation, then apply stemming\n",
        "    for word in tokenized_tweet:\n",
        "        if word not in stopwords_english and word not in string.punctuation:\n",
        "            stemmed_word = stemmer.stem(word)  # Reduce word to its root form\n",
        "            tweets_clean.append(stemmed_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjioQEB12koh",
        "outputId": "60037140-59e8-45e4-c3b7-4e8e44ccdfc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        }
      },
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\"\"\"\n",
        "Use the function to iterate through the original dataframe 'df' and preprocess each tweet into a new column \"preprocessed_tweets\". Pseudocode below.\n",
        "\n",
        "for i in len(df):\n",
        "  df[\"preprocessed_tweet\"][i] = preprocess_tweet(df[\"text\"][i])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Ensure that 'df' is loaded with the tweet data as assumed\n",
        "# Define the necessary NLP tools\n",
        "tokenizer = TweetTokenizer()\n",
        "stopwords_english = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes tweets for further analysis.\n",
        "    Input:\n",
        "        tweet: String, raw tweet text.\n",
        "    Output:\n",
        "        tweets_clean: List of strings, cleaned and stemmed words from the tweet.\n",
        "    \"\"\"\n",
        "    # Strip retweet artifacts\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    # Eliminate URLs\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "    # Remove leading byte order marks or similar artifacts\n",
        "    tweet = tweet.lstrip('b\\'\"')\n",
        "\n",
        "    # Break the tweet into tokens\n",
        "    tokenized_tweet = tokenizer.tokenize(tweet)\n",
        "\n",
        "    # Clean tokens by removing stopwords and punctuation, and apply stemming\n",
        "    tweets_clean = [stemmer.stem(word) for word in tokenized_tweet if word.lower() not in stopwords_english and word not in string.punctuation]\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "# Applying the preprocessing function to every tweet in the dataframe\n",
        "df['preprocessed_tweets'] = df['text'].apply(preprocess_tweet)\n",
        "\n",
        "# Print out a part of the dataframe to verify changes\n",
        "print(df[['text', 'preprocessed_tweets']])\n",
        "\n",
        "# Display the DataFrame to check the updated column\n",
        "display(df)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  \\\n",
            "0     b'And so the robots spared humanity ... https:...   \n",
            "1     b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...   \n",
            "2         b'@waltmossberg @mims @defcon_5 Et tu, Walt?'   \n",
            "3                   b'Stormy weather in Shortville ...'   \n",
            "4     b\"@DaveLeeBBC @verge Coal is dying due to nat ...   \n",
            "...                                                 ...   \n",
            "2814               b'That was a total non sequitur btw'   \n",
            "2815  b'Great Voltaire quote, arguably better than T...   \n",
            "2816  b'I made the volume on the Model S http://t.co...   \n",
            "2817  b\"Went to Iceland on Sat to ride bumper cars o...   \n",
            "2818  b'Please ignore prior tweets, as that was some...   \n",
            "\n",
            "                                    preprocessed_tweets  \n",
            "0                            [robot, spare, human, ...]  \n",
            "1     [@forin2020, @waltmossberg, @mim, @defcon_5, e...  \n",
            "2        [@waltmossberg, @mim, @defcon_5, et, tu, walt]  \n",
            "3                      [stormi, weather, shortvil, ...]  \n",
            "4     [@daveleebbc, @verg, coal, die, due, nat, ga, ...  \n",
            "...                                                 ...  \n",
            "2814                        [total, non, sequitur, btw]  \n",
            "2815  [great, voltair, quot, arguabl, better, twain,...  \n",
            "2816                               [made, volum, model]  \n",
            "2817  [went, iceland, sat, ride, bumper, car, ice, c...  \n",
            "2818  [pleas, ignor, prior, tweet, someon, pretend, ...  \n",
            "\n",
            "[2819 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                      id           created_at  \\\n",
              "0     849636868052275200  2017-04-05 14:56:29   \n",
              "1     848988730585096192  2017-04-03 20:01:01   \n",
              "2     848943072423497728  2017-04-03 16:59:35   \n",
              "3     848935705057280001  2017-04-03 16:30:19   \n",
              "4     848416049573658624  2017-04-02 06:05:23   \n",
              "...                  ...                  ...   \n",
              "2814  142881284019060736  2011-12-03 08:22:07   \n",
              "2815  142880871391838208  2011-12-03 08:20:28   \n",
              "2816  142188458125963264  2011-12-01 10:29:04   \n",
              "2817  142179928203460608  2011-12-01 09:55:11   \n",
              "2818         15434727182  2010-06-04 18:31:57   \n",
              "\n",
              "                                                   text  \\\n",
              "0     b'And so the robots spared humanity ... https:...   \n",
              "1     b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...   \n",
              "2         b'@waltmossberg @mims @defcon_5 Et tu, Walt?'   \n",
              "3                   b'Stormy weather in Shortville ...'   \n",
              "4     b\"@DaveLeeBBC @verge Coal is dying due to nat ...   \n",
              "...                                                 ...   \n",
              "2814               b'That was a total non sequitur btw'   \n",
              "2815  b'Great Voltaire quote, arguably better than T...   \n",
              "2816  b'I made the volume on the Model S http://t.co...   \n",
              "2817  b\"Went to Iceland on Sat to ride bumper cars o...   \n",
              "2818  b'Please ignore prior tweets, as that was some...   \n",
              "\n",
              "                                    preprocessed_tweets  \n",
              "0                            [robot, spare, human, ...]  \n",
              "1     [@forin2020, @waltmossberg, @mim, @defcon_5, e...  \n",
              "2        [@waltmossberg, @mim, @defcon_5, et, tu, walt]  \n",
              "3                      [stormi, weather, shortvil, ...]  \n",
              "4     [@daveleebbc, @verg, coal, die, due, nat, ga, ...  \n",
              "...                                                 ...  \n",
              "2814                        [total, non, sequitur, btw]  \n",
              "2815  [great, voltair, quot, arguabl, better, twain,...  \n",
              "2816                               [made, volum, model]  \n",
              "2817  [went, iceland, sat, ride, bumper, car, ice, c...  \n",
              "2818  [pleas, ignor, prior, tweet, someon, pretend, ...  \n",
              "\n",
              "[2819 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-232d5be9-acd4-4365-b914-134b1ecf54f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>preprocessed_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>849636868052275200</td>\n",
              "      <td>2017-04-05 14:56:29</td>\n",
              "      <td>b'And so the robots spared humanity ... https:...</td>\n",
              "      <td>[robot, spare, human, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>848988730585096192</td>\n",
              "      <td>2017-04-03 20:01:01</td>\n",
              "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
              "      <td>[@forin2020, @waltmossberg, @mim, @defcon_5, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>848943072423497728</td>\n",
              "      <td>2017-04-03 16:59:35</td>\n",
              "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
              "      <td>[@waltmossberg, @mim, @defcon_5, et, tu, walt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>848935705057280001</td>\n",
              "      <td>2017-04-03 16:30:19</td>\n",
              "      <td>b'Stormy weather in Shortville ...'</td>\n",
              "      <td>[stormi, weather, shortvil, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>848416049573658624</td>\n",
              "      <td>2017-04-02 06:05:23</td>\n",
              "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
              "      <td>[@daveleebbc, @verg, coal, die, due, nat, ga, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2814</th>\n",
              "      <td>142881284019060736</td>\n",
              "      <td>2011-12-03 08:22:07</td>\n",
              "      <td>b'That was a total non sequitur btw'</td>\n",
              "      <td>[total, non, sequitur, btw]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2815</th>\n",
              "      <td>142880871391838208</td>\n",
              "      <td>2011-12-03 08:20:28</td>\n",
              "      <td>b'Great Voltaire quote, arguably better than T...</td>\n",
              "      <td>[great, voltair, quot, arguabl, better, twain,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2816</th>\n",
              "      <td>142188458125963264</td>\n",
              "      <td>2011-12-01 10:29:04</td>\n",
              "      <td>b'I made the volume on the Model S http://t.co...</td>\n",
              "      <td>[made, volum, model]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2817</th>\n",
              "      <td>142179928203460608</td>\n",
              "      <td>2011-12-01 09:55:11</td>\n",
              "      <td>b\"Went to Iceland on Sat to ride bumper cars o...</td>\n",
              "      <td>[went, iceland, sat, ride, bumper, car, ice, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2818</th>\n",
              "      <td>15434727182</td>\n",
              "      <td>2010-06-04 18:31:57</td>\n",
              "      <td>b'Please ignore prior tweets, as that was some...</td>\n",
              "      <td>[pleas, ignor, prior, tweet, someon, pretend, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2819 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-232d5be9-acd4-4365-b914-134b1ecf54f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-232d5be9-acd4-4365-b914-134b1ecf54f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-232d5be9-acd4-4365-b914-134b1ecf54f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e2782e0c-8d05-40f7-a302-63d3c7aea84a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e2782e0c-8d05-40f7-a302-63d3c7aea84a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e2782e0c-8d05-40f7-a302-63d3c7aea84a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2819,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 218640394647883936,\n        \"min\": 15434727182,\n        \"max\": 849636868052275200,\n        \"num_unique_values\": 2819,\n        \"samples\": [\n          232366724064874497,\n          526192897247285248,\n          654409700801310720\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 2819,\n        \"samples\": [\n          \"2012-08-06 06:45:18\",\n          \"2014-10-26 02:05:37\",\n          \"2015-10-14 21:33:25\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2819,\n        \"samples\": [\n          \"b'RT @SpaceX: Success! Congrats @NASA on @MarsCuriosity!'\",\n          \"b'RT @SpaceX: [PHOTO] Splashdown! Dragon lands in the Pacific Ocean, carrying 3,276 lbs of cargo &amp; science samples. http://t.co/AZzCFISeDI'\",\n          \"b'RT @WIRED: Breaking: Tesla just made its cars autonomous, kinda http://t.co/1jxVX9k7rO'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preprocessed_tweets\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOFlGR0y2koi"
      },
      "source": [
        "## Part 1.3 Create a Levensthein Distance Formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km5cU90S2koj"
      },
      "source": [
        "Recall that Edit Distance is the similarity between two words represented numericallly. Levenstehin distance is one of the most common algorithms used in calculating the edit distance between two words.\n",
        "\n",
        "Create your own simple Levensthein distance function. Then return the results of the distance between two words: _stemming_ and _lemmatization_.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leven_dist(string1, string2):\n",
        "    '''\n",
        "    Computes the Levenshtein distance between two strings. This distance is the\n",
        "    minimum number of single-character edits (insertions, deletions, or substitutions)\n",
        "    required to change one word into the other.\n",
        "    input:\n",
        "        string1: First string for comparison\n",
        "        string2: Second string for comparison\n",
        "    output:\n",
        "        Integer representing the edit distance between string1 and string2\n",
        "    '''\n",
        "    # Base case: if the first string is empty, return the length of the second string\n",
        "    if not string1:\n",
        "        return len(string2)\n",
        "    # Base case: if the second string is empty, return the length of the first string\n",
        "    if not string2:\n",
        "        return len(string1)\n",
        "\n",
        "    # Check if the last characters of the strings are the same\n",
        "    if string1[-1] == string2[-1]:\n",
        "        substitution_cost = 0\n",
        "    else:\n",
        "        substitution_cost = 1\n",
        "\n",
        "    # Calculate distances for three possible operations: insertion, deletion, substitution\n",
        "    return min(\n",
        "        leven_dist(string1[:-1], string2) + 1,  # Deletion from string1\n",
        "        leven_dist(string1, string2[:-1]) + 1,  # Insertion into string1\n",
        "        leven_dist(string1[:-1], string2[:-1]) + substitution_cost  # Substitution\n",
        "    )\n",
        "\n",
        "# Example usage of the function\n",
        "string1 = 'stemming'\n",
        "string2 = 'lemmatization'\n",
        "edit_distance = leven_dist(string1, string2)\n",
        "print(\"Your Levenshtein Distance is: \", edit_distance)\n",
        "\n"
      ],
      "metadata": {
        "id": "teSaHxFCykFP",
        "outputId": "13b810b7-94bf-4d12-899f-79bb97ec302b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Levenshtein Distance is:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5gh6B-2kok"
      },
      "source": [
        "### Expected output:\n",
        "Your Levensthein Distance is: 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3PHkeRY2kol"
      },
      "source": [
        "#end of assignment#\n",
        "                                            \n",
        "Source:\n",
        "Natural Language preprocessing, deeplearning.ai\n",
        "Twitter API documentation\n",
        "Wikipedia: Levensthein Distance\n",
        "Chapter 2 (Jurafsky and Martin)\n",
        "                        \n"
      ]
    }
  ]
}